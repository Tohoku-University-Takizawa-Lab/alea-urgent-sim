HEADER1=Basic configuration
baudRate=double|Bandwidth.
entities=int|Total count of Job Submission System.
visualize=boolean|Visualize simulation|May slow down the simulation. Use only for testing or for obtaining graphical output.
reqs=boolean|Specific job requirements
estimates=boolean|Job runtime estimates
failures=boolean|Failure trace|If available
HEADER2=Simulation data
meta=boolean|Meta|Defines where to look for simulation data.
data=boolean|Data|Defines whether simulation data are outside of project folder.
useHeap=boolean|Use heap to store schedule-data|Should be true, as heap is faster than the default array.
path=String|Path|Used only when executed on a real cluster (do not change this variable).
HEADER3=Runtime estimates
useAvgLength=boolean|Average job length
useLastLength=boolean|Last job runtime
HEADER4=Scheduling strategy
fair_weight=int|The weight of fairness criterion
multiplicator=int|Multiplicator|Multiplies the number of iterations of opt. algorithms.
use_RAM=boolean|Follow job's RAM requirements
use_fairshare=boolean|Enable fair-share
use_fairshare_WAIT=boolean|Total user wait time in fair-share
use_fairshare_RAM=boolean|Fair-share counting in RAM as well as CPU time
use_compresion=boolean|Schedule compression upon early job completion
use_tsafrir=boolean|Tsafrir's esimates|If available in the data set
use_speeds=boolean|Machine's speeds to adjust job execution time
useEventOpt=boolean|On-demand LS-based optimization
HEADER5=Limits
time_limit=int|Max time limit for optimization algorithm
on_demand_time_limit=int|Max time limit for on-demand schedule optimization
sld_tresh=double|Bounded slowdown's threshold|Typically 10 seconds.
gap_length=int|The minimal length of a gap in schedule since the "on demand" optimization was executed (in seconds)
HEADER6=Factors
runtime_minimizer=double|Factor to decrease job's runtime
runtime_multiplicator=int|Runtime multiplicator factor|Factor by which the previous runtime is increased when historical estimates are used.
arrival_rate_multiplier=double|Rate multiplier used to compress job inter-arrival times|1.0 = original, 2.0 = double speed
HEADER7=Advanced configuration
multiply_sums=boolean|Multiply sum of CPU and RAM in fair-share
use_MAX=boolean|MAX of CPU and RAM usage in fair-share
use_SQRT=boolean|SQRT of CPU and RAM usage in fair-share
sum_multiplications=boolean|Sum multiplications of CPU and RAM in fairhshare|((RAM*CPUt)_job1 + (RAM*CPUt)_job2 + ... + (RAM*CPUt)_jobN)
use_anti_starvation=boolean|Anti-starvation technique based on resource pre-allocation
use_resource_spec_packing=boolean|Jobs's resource specification can be adjusted to increase througput|So called nodespec packing option as seen in PBS Pro, etc.
use_queues=boolean|Use several different queues in the system|Such queues must be specified in a seperate file, along with job and machine descriptions.
HEADER8=Data sets
data_sets=String[]|Data set name(s) are stored in this list|Typically, the data are expected to be in a "$PATH/data-set/" directory, where $PATH is the path to where the Alea directory is. Therefore, this directory should contain both ./Alea and ./data-set directories. Files describing machines should be placed in a file named e.g., "metacentrum.mwf.machines". Similarly machine failures (if simulated) should be placed in a file called e.g., "metacentrum.mwf.failures". Please read carefully the copyright note when using public workload traces!
total_gridlet=int[]|Number of gridlets in data set
skip=int|Number of jobs that should be skipped in the data set
HEADER9=Algorithms
algorithms=int[]|IDs of the algorithms to be used|FCFS = 0, EDF = 1, EASY = 2, AgresiveBF = 3, CONS compression = 4, PBS PRO = 5, SJF = 6, FairShareFCFS = 7, FairShareMetaBackfilling = 8, FairShareCONS = 9, BestGap = 10, BestGap+RandomSearch = 11, 18 = CONS+Tabu Search, 19 = CONS + Gap Search, 20 = CONS + RandomSearch, CONS no compression = 21
HEADER10=Advanced configuration for each algorithm selected
fairw=int[]|The weight of the fairness criteria in objective function
stradej=boolean[]|Anti-starvation technique based on resource pre-allocation
packuj=boolean[]|Jobs's resource specification can be adjusted to increase througput|So called nodespec packing option as seen in PBS Pro, etc.
skipuj=int[]|Number of jobs that should be skipped in the data set
HEADER11=Plugins
plugins=String[]|List of available plugins
plugin.result_header=String|Result header|The header that should appear in the csv file
