HEADER1=Data sets
data_sets=String[]|Data set name(s) are stored in this list|Typically, the data are expected to be in a "$PATH/data-set/" directory, where $PATH is the path to where the Alea directory is. Therefore, this directory should contain both ./Alea and ./data-set directories. Files describing machines should be placed in a file named e.g., "metacentrum.mwf.machines". Similarly machine failures (if simulated) should be placed in a file called e.g., "metacentrum.mwf.failures". Please read carefully the copyright note when using public workload traces!|
HEADER2=Algorithms
algorithms=int[]|IDs of the algorithms to be used|FCFS = 0, EDF = 1, EASY Backfilling = 2, Agresive Backfilling = 3, CONS compression = 4, PBS PRO = 5, SJF = 6, Fairshare FCFS = 7, Fairshare Meta Backfilling = 8, Fairshare CONS = 9, BestGap = 10, BestGap + RandomSearch = 11, Fairshare Optimized Meta Backfilling = 12, CONS + Tabu Search = 18, CONS + Gap Search = 19, CONS + RandomSearch = 20, CONS no compression = 21, Fairshare EASY Backfilling = 22, Backfilling CONS Fair = 23, CONS with Random Search optimization algorithm and Gap Search as opt. algorithm used to fix schedule after schedule = 204, CONS with Weighted Random Search optimization algorithm and Weighted Random Search as opt. algorithm used to fix schedule after schedule = 207|
HEADER3=Configuration
total_gridlet=int[]|Number of gridlets in data set||
meta=boolean|Meta|Defines where to look for simulation data.|
data=boolean|Data|Defines whether simulation data are outside of project folder.|
path=String|Path|Used only when executed on a real cluster (do not change this variable).|
visualize=boolean|Visualize simulation|May slow down the simulation. Use only for testing or for obtaining graphical output.|
reqs=boolean|Specific job requirements|Obsolete parameter, should be set to false all the time.|
failures=boolean|Failure trace|Obsolete parameter, should be set to false. Otherwise, a separate file describing machine failures must be provided.|
use_speeds=boolean|Machines' speeds to adjust job execution time|Obsolete parameter, should be set to false all the time.|
use_heap=boolean|Use heap to store schedule-data|Should be true, as heap is faster than the default array.|
HEADER4=Multi-queue configuration
use_queues=boolean|Use several different queues in the system|Will create multiple queues in the system. Use only if there is a separate queue description file which corresponds to the queues specified in the data set. Only effective when all algorithms support multi-queue scheduling.|0,2,3,7,8,12,22
by_queue=boolean|Queue-by-queue usage of multiple queues|Defines whether queues will be used separately (queue-by-queue in a defined priority order) or they will only be used to guard queue-limits.(false = all jobs stored within 1 major queue (separate queue limits are still being used). Only effective when all algorithms support multi-queue scheduling.|0,2,3,7,8,12,22
HEADER5=Runtime estimates
estimates=boolean|Job runtime estimates||
use_AvgLength=boolean|Average job length||
use_LastLength=boolean|Last job runtime||
use_tsafrir=boolean|Tsafrir's estimates|If available in the data set.|
HEADER6=Scheduling strategy
use_RAM=boolean|Follow job's RAM requirements|Use only when all data sets being used provide information concerning used/available RAM. RAM is a per-node parameter, i.e., problems may occur if a data set does not properly specify a job's per-node requirements. In that case, a modification of job loader (e.g., SWFLoader) will be necessary to properly "decode" a job's request.|
useEventOpt=boolean|On-demand LS-based optimization|set true to use "on demand" schedule optimization when early job completions appear.|
multiplicator=int|Multiplicator|Multiplies the number of iterations of opt. algorithms.|
HEADER7=Fairshare related configuration
use_fairshare=boolean|Enable fair-share||7,8,9,12,22
use_fairshare_RAM=boolean|Fair-share counting in RAM as well as CPU time||7,8,9,12,22
use_fairshare_WAIT=boolean|Total user wait time in fair-share||7,8,9,12,22
multiply_sums=boolean|Used to define the type of fair-sharing algorithm. Multiply sum of CPU and RAM in fair-share|Only effective when scheduling algorithm supports fair-share based job ordering.|7,8,9,12,22
use_MAX=boolean|MAX of CPU and RAM usage in fair-share|Uses MAX(RAM*CPUt). Only effective when scheduling algorithm supports fair-share based job ordering.|7,8,9,12,22
use_SQRT=boolean|SQRT of CPU and RAM usage in fair-share|Uses 1- SQRT((1-RAM)*(1-CPUt)). Used to define the type of fair-sharing algorithm. Only effective when scheduling algorithm supports fair-share based job ordering.|7,8,9,12,22
sum_multiplications=boolean|Sum multiplications of CPU and RAM in fairhshare. Used to define the type of fair-sharing algorithm.|((RAM*CPUt)_job1 + (RAM*CPUt)_job2 + ... + (RAM*CPUt)_jobN). When setting to true, the first two parameters in this section must be also set to true.|
HEADER8=Limits and Factors
time_limit=int|Max time limit for optimization algorithm||
on_demand_time_limit=int|Max time limit for on-demand schedule optimization||
sld_tresh=double|Bounded slowdown's threshold|Typically 10 seconds.|
gap_length=int|The minimal length of a gap in schedule since the "on demand" optimization was executed (in seconds)||
runtime_minimizer=double|Factor to decrease job's runtime|Used together with arrival_rate_multiplier to influence the length of job (swf files only).|
runtime_multiplicator=int|Runtime multiplicator factor|Factor by which the previous runtime is increased when historical estimates are used.|
arrival_rate_multiplier=double|Rate multiplier used to compress job inter-arrival times|1.0 = original, 2.0 = double speed|
HEADER9=Advanced configuration
baudRate=double|Bandwidth of interconnecting network.||
entities=int|Total count of Job Submission System.||
HEADER10=Configuration for each algorithm selected
weight_of_fairness=int[]|The weight of the fairness criteria in objective function||10,11,18,19,20,204,207
use_anti_starvation=boolean[]|Anti-starvation technique based on resource pre-allocation||8,12
use_resource_spec_packing=boolean[]|Jobs's resource specification can be adjusted to increase througput|So called nodespec packing option as seen in PBS Pro, etc.|
skip=int[]|Number of jobs that should be skipped in the data set||
HEADER11=Scheduling metrics as plugins
plugins=String[]|List of available plugins|Enter a simple class name if the class is in the plugins subpackage of Alea. Otherwise, enter a fully qualified class name.|
plugin.result_header=String|Results header|The header that should appear in the csv file.|
