HEADER1=Basic configuration
visualize=boolean|Visualize simulation|May slow down the simulation. Use only for testing or for obtaining graphical output.
reqs=boolean|Specific job requirements
failures=boolean|Failure trace|If available.
use_queues=boolean|Use several different queues in the system|Such queues must be specified in a seperate file, along with job and machine descriptions.
by_queue=boolean|Queue-by-queue usage of multiple queues|Defines whether queues will be used separately (queue-by-queue in a defined priority order) or they will only be used to guard queue-limits.(false = all jobs stored within 1 major queue (separate queue limits are still being used)
use_speeds=boolean|Machines' speeds to adjust job execution time
use_heap=boolean|Use heap to store schedule-data|Should be true, as heap is faster than the default array.
HEADER2=Runtime estimates
estimates=boolean|Job runtime estimates
use_AvgLength=boolean|Average job length
use_LastLength=boolean|Last job runtime
use_tsafrir=boolean|Tsafrir's estimates|If available in the data set.
HEADER3=Scheduling strategy
use_RAM=boolean|Follow job's RAM requirements
use_fairshare=boolean|Enable fair-share
use_fairshare_WAIT=boolean|Total user wait time in fair-share
use_fairshare_RAM=boolean|Fair-share counting in RAM as well as CPU time
use_compresion=boolean|Schedule compression upon early job completion
multiply_sums=boolean|Multiply sum of CPU and RAM in fair-share
use_MAX=boolean|MAX of CPU and RAM usage in fair-share|Uses MAX(RAM*CPUt)
use_SQRT=boolean|SQRT of CPU and RAM usage in fair-share|Uses 1- SQRT((1-RAM)*(1-CPUt))
sum_multiplications=boolean|Sum multiplications of CPU and RAM in fairhshare|((RAM*CPUt)_job1 + (RAM*CPUt)_job2 + ... + (RAM*CPUt)_jobN)
use_anti_starvation=boolean|Anti-starvation technique based on resource pre-allocation
use_resource_spec_packing=boolean|Jobs's resource specification can be adjusted to increase througput|So called nodespec packing option as seen in PBS Pro, etc.
useEventOpt=boolean|On-demand LS-based optimization|set true to use "on demand" schedule optimization when early job completions appear
multiplicator=int|Multiplicator|Multiplies the number of iterations of opt. algorithms.
HEADER4=Limits and Factors
fair_weight=int|The weight of fairness criterion
time_limit=int|Max time limit for optimization algorithm
on_demand_time_limit=int|Max time limit for on-demand schedule optimization
sld_tresh=double|Bounded slowdown's threshold|Typically 10 seconds.
gap_length=int|The minimal length of a gap in schedule since the "on demand" optimization was executed (in seconds)
runtime_minimizer=double|Factor to decrease job's runtime|Used together with arrival_rate_multiplier to influence the length of job (swf files only)
runtime_multiplicator=int|Runtime multiplicator factor|Factor by which the previous runtime is increased when historical estimates are used.
arrival_rate_multiplier=double|Rate multiplier used to compress job inter-arrival times|1.0 = original, 2.0 = double speed
HEADER5=Advanced configuration
baudRate=double|Bandwidth of interconnecting network.
entities=int|Total count of Job Submission System.
HEADER6=Data sets
data_sets=String[]|Data set name(s) are stored in this list|Typically, the data are expected to be in a "$PATH/data-set/" directory, where $PATH is the path to where the Alea directory is. Therefore, this directory should contain both ./Alea and ./data-set directories. Files describing machines should be placed in a file named e.g., "metacentrum.mwf.machines". Similarly machine failures (if simulated) should be placed in a file called e.g., "metacentrum.mwf.failures". Please read carefully the copyright note when using public workload traces!
total_gridlet=int[]|Number of gridlets in data set
skip=int|Number of jobs that should be skipped in the data set
meta=boolean|Meta|Defines where to look for simulation data.
data=boolean|Data|Defines whether simulation data are outside of project folder.
path=String|Path|Used only when executed on a real cluster (do not change this variable).
HEADER7=Algorithms
algorithms=int[]|IDs of the algorithms to be used|FCFS = 0, EDF = 1, EASY = 2, AgresiveBF = 3, CONS compression = 4, PBS PRO = 5, SJF = 6, FairShareFCFS = 7, FairShareMetaBackfilling = 8, FairShareCONS = 9, BestGap = 10, BestGap+RandomSearch = 11, FairShareOptimizedMetaBackfilling = 12, 18 = CONS+Tabu Search, 19 = CONS + Gap Search, 20 = CONS + RandomSearch, CONS no compression = 21
HEADER8=Advanced configuration for each algorithm selected
fairw=int[]|The weight of the fairness criteria in objective function
stradej=boolean[]|Anti-starvation technique based on resource pre-allocation
do_pack=boolean[]|Jobs's resource specification can be adjusted to increase througput|So called nodespec packing option as seen in PBS Pro, etc.
skipuj=int[]|Number of jobs that should be skipped in the data set
HEADER9=Scheduling metrics as plugins
plugins=String[]|List of available plugins|Enter a simple class name if the class is in the plugins subpackage of Alea. Otherwise, enter a fully qualified class name.
plugin.result_header=String|Results header|The header that should appear in the csv file
